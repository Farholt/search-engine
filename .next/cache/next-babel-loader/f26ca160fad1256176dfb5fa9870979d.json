{"ast":null,"code":"const search = (req, res) => {\n  const query = req.body.query;\n\n  const fs = require('fs');\n\n  const str = fs.readFileSync('shared/json/pages.json').toString();\n  const obj = JSON.parse(str);\n  res.json(obj['7400_series']);\n  /* \r\n  \r\n    Steps:\r\n    1) Generate index for all wikipedia pages. In e.g. SQL db or for smaller sets in main memory\r\n    2) The word list will be stored as integer values so mapping of the words needs to be done and it will then be given a key value and\r\n  \r\n    Word to id\r\n    \r\n    int getIdForWord(String word)\r\n      if(wordToId.containsKey(word))\r\n        // word found in hashmap\r\n        return wordToId.get(word)\r\n      else\r\n        // add missing word to hashmap\r\n        int id = wordToId.size()\r\n        wordToId.put(word, id)\r\n        return id\r\n      Generating a word list\r\n    - Read the bag-of-words data files in the dataset Links/Words and create one page for each file\r\n    - The Page object shall contain the url to the page and the words in the page\r\n      Searching\r\n    - Convert the search query to a sequence of word id values using the getIdForWord\r\n    - Then we return all the documents that has any of the word id values in their word list\r\n    - This generates a search result, the next step is to sort (rank the different pages based on relevance) it\r\n      Sorting/Ranking\r\n    Order By Content \r\n    (Content-Based Ranking)\r\n    - It uses three different metrics: \r\n      Word frequency (how many times the word occur)\r\n      Document location\r\n      Word distance (Word Distance is based on the idea that queries using multiple words often finds more relevant if the words appear close together on the page)\r\n    (Basic Query Structure)\r\n    - Create an empty list that the result is placed in (Array?)\r\n    - For every page\r\n      Calculate the metric value for each of the metrics used\r\n    - Normalize the metric values so all metrics are between 0 and 1\r\n    - Calculate the final score that is a weighted sum of all normalized metric values\r\n      Code e.g.\r\n      [Algorithm]\r\n    void query(String query)\r\n      result = new List()\r\n      Score scores\r\n        // Calculate score for each page in the pages database\r\n      for (i = 0 to pagedb.noPages() - 1)\r\n        Page p = pagedb.get(i) // this is the page object\r\n        [Word Frequency Metric function]\r\n        scores.content[i] = getFrequencyScore(p, query) // Words\r\n        [Document Location Metric]\r\n        scores.location[i] = getLocationScore(p, query) // Links\r\n        // normalize the scores\r\n      [Normalize function]\r\n      normalize(scores.content, false) // Words\r\n      normalize(scores.location, true) // Links\r\n        // generate result list\r\n      for (i = 0 to pageDb.noPages() - 1)\r\n        Page p = pagedb.get(i)\r\n        double score = 1.0 * scores.content[i] + 0.5 * scores.location[i]\r\n        result.add(Score(p, score))\r\n        // sort list with highest score first\r\n      sort(result)\r\n        // return result list\r\n      return result\r\n      Normalization\r\n    - The metric values are to have a score between 0 and 1. Low is bad, high is good.\r\n    - What's needed is a function that converts the metric value from void query that converts the value to a score between 0 and 1, regardless of if high values are good or bad\r\n    \r\n    Code e.g.\r\n      [Normalization]\r\n    void normalize(double[] scores, bool smallIsBetter)\r\n      if(smallIsBetter)\r\n        // smaller values shall be inverted to higher values\r\n        // and scale between 0 and 1\r\n        // find minimum value in the array\r\n        double min = Min(scores)\r\n          for(i = 0 to scores.length() - 1)\r\n          scores[i] = min / Max(scores[i], 0.00001)\r\n      else\r\n        // higher values must be scaled between 0 and 1\r\n        // find max value in the array\r\n        double max = Max(scores)\r\n          for(i = 0 to scores.length() - 1)\r\n          scores[i] = scores[i] / max\r\n      [Word Frequency Metric]\r\n    - Returns a metric score based on how many times the words in the query appears on the page\r\n      The algorithm works something like this\r\n      1. Declare a score variable and set it to 0\r\n      2. Split the search into a list of words (this applies only when the search query has more than 1 word)\r\n      3. For each word in the query\r\n          1. Convert the word string to an id integer\r\n          2. For each Page: \r\n            1. For each word in the page:\r\n                1. Increase score by 1 if the current word id matches the query word id\r\n      4. Return the score\r\n    HIGHER SCORES BETTER\r\n    Basically a counter for how many times the word occurs in the list?\r\n      [Document Location Metric]\r\n    - The doucments location means the search query's location on the page\r\n    - It builds on the idea that if a word is relevant for the page, it appears close to the top of the page (perhaps even in the title)\r\n    \r\n    It works something like this: \r\n      1. Declare a score variable and set it to 0\r\n      2. Split the search into a list of words (this applies only when the search query has more than 1 word)\r\n      3. For each word in the query\r\n          1. Convert the word string to an id integer\r\n          2. For each Page:\r\n              1. For each Word in the page: \r\n                  1. Increase the score by current index (POSITION IN HASHMAP/LIST), if the current id matches the query word id matches the query word\r\n              2. If the word isn't found increase the score by a high value (100000)\r\n      4. Return the score\r\n      LOWER SCORES ARE BETTER\r\n  \r\n    [Word Distance Metric]\r\n    - Word Distance is based on the idea that queries using multiple words often finds more relevant if the words appear close together on the page\r\n    - The metric therefore uses the distance between pair of words in the document\r\n    - It can only be used if we have two or more words in the query\r\n    \r\n    The code may look something like this:\r\n      1. Declare a score variable and set value to 0\r\n      2. Split the search query into a list of words\r\n      3. For each unique pair of words in the query: \r\n          1. Find location of word 1 using the Document Location Metric\r\n          2. Find location of word 2 using the Document Location Metric\r\n          3. Increase the score by the absolute value of indexWord1 minus indexWord2\r\n          4. If any or both words are missing in the document, increase the score by a high value (100000)\r\n      4. Return the scores\r\n      LOWER SCORES ARE BETTER\r\n        - This metric is quite heavy to calculate, there are several nested for-loops.\r\n        \r\n        \r\n    */\n\n  /* \r\n  \r\n      Inbound-Link Ranking\r\n      - A second approach we will use is Inbound-Link ranking\r\n      - Is different from Content-Based ranking in that it doesn't use the contents of a page\r\n      - Instead it uses information others have provided about a page, more specifically who has linked to the page\r\n      - The idea is that bad pages are less likely to be linked to, and pages with good content have numerous other pages linking to them\r\n      - Each page object must now also contain all outgoing links from that page\r\n      - Links are stored in a separate file in the Wikipedia data set zip file\r\n        Which metric to use?\r\n      - There are no universal metric that consistently give good results\r\n      - Often we have to combine different metrics\r\n      - We can also experiment with giving different weights to different metrics\r\n      - The total score can for examble be calculates as (assuming metric scores are normalized)\r\n          totalScore = 1.0 * WordFrequency + 0.8 * DocumentLocation + 0.5 * WordDistance\r\n      - Two ways of doing the Inbound-Link ranking is: Simple Count and Google PageRank algorithm\r\n        Simple Count\r\n      - It simply counts how many other pages that link to the current page\r\n      - Easy to find\r\n      - For each existing page we iterate all the other pages and increase the score by 1 if the other page links to the current page\r\n      - This is often how academic papers are often ranked.\r\n      - A peper is more important if many other papers reference to it\r\n      \r\n      - This way alone is not useable\r\n      - It doesn't care about the search query, it only returns the page with most links to it\r\n      - It must be combined with CONTENT-BASED ranking\r\n      - A drawback with Simple Count is that it treats every inbound link equally\r\n      - Ideally, we would like to weight inbound links from high quality web sites higher\r\n      - This is dealt with in the PageRank algorithm\r\n        PageRank algorithm\r\n      - Invented by the founders of Google and is named after Larry Page\r\n      - Variation of it are now used by many large search engines\r\n      - The basic idea is that every page is assigned a score that indicates how important the page is\r\n      - The importance of a page is calculated from the importance of all other pages linking to it\r\n      - The importance score is then used to weight inbound links to a page\r\n        The Theory\r\n      - The algorithm calculates the probability that someone randomly clicking on links will arrive at a certain page\r\n      - The more inbound links a page has from other popular pages, the more likely it is that someone will visit the page by pure chance\r\n      - If the user keeps clicking forever he will eventually reach every page\r\n      - Since users stop surfing after a while, PageRank has a damping factor of 0.85 indicating that there is a 85% chance that a user will continue clicking from a page\r\n  \r\n  */\n\n  /* Indexing */\n  // Give each word a unique value, e.g. use hash codes\n  // Store in a linked list of integers instead so that it has a key\n\n  /* Searching (when we have index) */\n  // When we search for a single word the return is a list containing all documents that has the word\n  // Multiple words returns a list of documents that has any of the words in them (perhaps all documents with all the words as well ). Really depends on the purpose of the search engine.\n\n  /* Ranking */\n  // Here comes the sorting of the result...\n  // The sort is based on how relevant each document is to the search query (const query)\n  // This needs to be done by generating a numeric score for each document's relevance\n  // The score is based on a combination of several metrics. The metric can be tweaked (if needed) to hopefully work better for a specific search engine\n};\n\nexport default search;","map":{"version":3,"sources":["C:/Users/fredr/Documents/Universitet/HT20/2DV515 - Web Intelligence/A3/pages/api/search.ts"],"names":["search","req","res","query","body","fs","require","str","readFileSync","toString","obj","JSON","parse","json"],"mappings":"AAIA,MAAMA,MAAM,GAAG,CAACC,GAAD,EAAsBC,GAAtB,KAA+C;AAC5D,QAAMC,KAAU,GAAGF,GAAG,CAACG,IAAJ,CAASD,KAA5B;;AAEA,QAAME,EAAE,GAAGC,OAAO,CAAC,IAAD,CAAlB;;AACA,QAAMC,GAAW,GAAGF,EAAE,CAACG,YAAH,CAAgB,wBAAhB,EAA0CC,QAA1C,EAApB;AACA,QAAMC,GAAW,GAAGC,IAAI,CAACC,KAAL,CAAWL,GAAX,CAApB;AAEAL,EAAAA,GAAG,CAACW,IAAJ,CAASH,GAAG,CAAC,aAAD,CAAZ;AAEA;AACF;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAwBE;AACF;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAME;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACD,CA9ND;;AAgOA,eAAeV,MAAf","sourcesContent":["import { strict } from 'assert'\r\nimport { forEach } from 'list'\r\nimport { NextApiRequest, NextApiResponse } from 'next'\r\n\r\nconst search = (req: NextApiRequest, res: NextApiResponse) => {\r\n  const query: any = req.body.query\r\n\r\n  const fs = require('fs')\r\n  const str: string = fs.readFileSync('shared/json/pages.json').toString()\r\n  const obj: object = JSON.parse(str)\r\n\r\n  res.json(obj['7400_series'])\r\n\r\n  /* \r\n  \r\n    Steps:\r\n    1) Generate index for all wikipedia pages. In e.g. SQL db or for smaller sets in main memory\r\n    2) The word list will be stored as integer values so mapping of the words needs to be done and it will then be given a key value and\r\n  \r\n    Word to id\r\n    \r\n    int getIdForWord(String word)\r\n      if(wordToId.containsKey(word))\r\n        // word found in hashmap\r\n        return wordToId.get(word)\r\n      else\r\n        // add missing word to hashmap\r\n        int id = wordToId.size()\r\n        wordToId.put(word, id)\r\n        return id\r\n\r\n    Generating a word list\r\n    - Read the bag-of-words data files in the dataset Links/Words and create one page for each file\r\n    - The Page object shall contain the url to the page and the words in the page\r\n\r\n    Searching\r\n    - Convert the search query to a sequence of word id values using the getIdForWord\r\n    - Then we return all the documents that has any of the word id values in their word list\r\n    - This generates a search result, the next step is to sort (rank the different pages based on relevance) it\r\n\r\n    Sorting/Ranking\r\n    Order By Content \r\n    (Content-Based Ranking)\r\n    - It uses three different metrics: \r\n      Word frequency (how many times the word occur)\r\n      Document location\r\n      Word distance (Word Distance is based on the idea that queries using multiple words often finds more relevant if the words appear close together on the page)\r\n    (Basic Query Structure)\r\n    - Create an empty list that the result is placed in (Array?)\r\n    - For every page\r\n      Calculate the metric value for each of the metrics used\r\n    - Normalize the metric values so all metrics are between 0 and 1\r\n    - Calculate the final score that is a weighted sum of all normalized metric values\r\n\r\n    Code e.g.\r\n\r\n    [Algorithm]\r\n    void query(String query)\r\n      result = new List()\r\n      Score scores\r\n\r\n      // Calculate score for each page in the pages database\r\n      for (i = 0 to pagedb.noPages() - 1)\r\n        Page p = pagedb.get(i) // this is the page object\r\n        [Word Frequency Metric function]\r\n        scores.content[i] = getFrequencyScore(p, query) // Words\r\n        [Document Location Metric]\r\n        scores.location[i] = getLocationScore(p, query) // Links\r\n\r\n      // normalize the scores\r\n      [Normalize function]\r\n      normalize(scores.content, false) // Words\r\n      normalize(scores.location, true) // Links\r\n\r\n      // generate result list\r\n      for (i = 0 to pageDb.noPages() - 1)\r\n        Page p = pagedb.get(i)\r\n        double score = 1.0 * scores.content[i] + 0.5 * scores.location[i]\r\n        result.add(Score(p, score))\r\n\r\n      // sort list with highest score first\r\n      sort(result)\r\n\r\n      // return result list\r\n      return result\r\n\r\n    Normalization\r\n    - The metric values are to have a score between 0 and 1. Low is bad, high is good.\r\n    - What's needed is a function that converts the metric value from void query that converts the value to a score between 0 and 1, regardless of if high values are good or bad\r\n    \r\n    Code e.g.\r\n\r\n    [Normalization]\r\n    void normalize(double[] scores, bool smallIsBetter)\r\n      if(smallIsBetter)\r\n        // smaller values shall be inverted to higher values\r\n        // and scale between 0 and 1\r\n        // find minimum value in the array\r\n        double min = Min(scores)\r\n\r\n        for(i = 0 to scores.length() - 1)\r\n          scores[i] = min / Max(scores[i], 0.00001)\r\n      else\r\n        // higher values must be scaled between 0 and 1\r\n        // find max value in the array\r\n        double max = Max(scores)\r\n\r\n        for(i = 0 to scores.length() - 1)\r\n          scores[i] = scores[i] / max\r\n\r\n    [Word Frequency Metric]\r\n    - Returns a metric score based on how many times the words in the query appears on the page\r\n\r\n    The algorithm works something like this\r\n      1. Declare a score variable and set it to 0\r\n      2. Split the search into a list of words (this applies only when the search query has more than 1 word)\r\n      3. For each word in the query\r\n          1. Convert the word string to an id integer\r\n          2. For each Page: \r\n            1. For each word in the page:\r\n                1. Increase score by 1 if the current word id matches the query word id\r\n      4. Return the score\r\n    HIGHER SCORES BETTER\r\n    Basically a counter for how many times the word occurs in the list?\r\n\r\n    [Document Location Metric]\r\n    - The doucments location means the search query's location on the page\r\n    - It builds on the idea that if a word is relevant for the page, it appears close to the top of the page (perhaps even in the title)\r\n    \r\n    It works something like this: \r\n      1. Declare a score variable and set it to 0\r\n      2. Split the search into a list of words (this applies only when the search query has more than 1 word)\r\n      3. For each word in the query\r\n          1. Convert the word string to an id integer\r\n          2. For each Page:\r\n              1. For each Word in the page: \r\n                  1. Increase the score by current index (POSITION IN HASHMAP/LIST), if the current id matches the query word id matches the query word\r\n              2. If the word isn't found increase the score by a high value (100000)\r\n      4. Return the score\r\n      LOWER SCORES ARE BETTER\r\n\r\n\r\n    [Word Distance Metric]\r\n    - Word Distance is based on the idea that queries using multiple words often finds more relevant if the words appear close together on the page\r\n    - The metric therefore uses the distance between pair of words in the document\r\n    - It can only be used if we have two or more words in the query\r\n    \r\n    The code may look something like this:\r\n      1. Declare a score variable and set value to 0\r\n      2. Split the search query into a list of words\r\n      3. For each unique pair of words in the query: \r\n          1. Find location of word 1 using the Document Location Metric\r\n          2. Find location of word 2 using the Document Location Metric\r\n          3. Increase the score by the absolute value of indexWord1 minus indexWord2\r\n          4. If any or both words are missing in the document, increase the score by a high value (100000)\r\n      4. Return the scores\r\n      LOWER SCORES ARE BETTER\r\n\r\n      - This metric is quite heavy to calculate, there are several nested for-loops.\r\n\r\n      \r\n\r\n      \r\n\r\n  */\r\n\r\n  /* \r\n  \r\n      Inbound-Link Ranking\r\n      - A second approach we will use is Inbound-Link ranking\r\n      - Is different from Content-Based ranking in that it doesn't use the contents of a page\r\n      - Instead it uses information others have provided about a page, more specifically who has linked to the page\r\n      - The idea is that bad pages are less likely to be linked to, and pages with good content have numerous other pages linking to them\r\n      - Each page object must now also contain all outgoing links from that page\r\n      - Links are stored in a separate file in the Wikipedia data set zip file\r\n\r\n      Which metric to use?\r\n      - There are no universal metric that consistently give good results\r\n      - Often we have to combine different metrics\r\n      - We can also experiment with giving different weights to different metrics\r\n      - The total score can for examble be calculates as (assuming metric scores are normalized)\r\n          totalScore = 1.0 * WordFrequency + 0.8 * DocumentLocation + 0.5 * WordDistance\r\n      - Two ways of doing the Inbound-Link ranking is: Simple Count and Google PageRank algorithm\r\n\r\n      Simple Count\r\n      - It simply counts how many other pages that link to the current page\r\n      - Easy to find\r\n      - For each existing page we iterate all the other pages and increase the score by 1 if the other page links to the current page\r\n      - This is often how academic papers are often ranked.\r\n      - A peper is more important if many other papers reference to it\r\n      \r\n      - This way alone is not useable\r\n      - It doesn't care about the search query, it only returns the page with most links to it\r\n      - It must be combined with CONTENT-BASED ranking\r\n      - A drawback with Simple Count is that it treats every inbound link equally\r\n      - Ideally, we would like to weight inbound links from high quality web sites higher\r\n      - This is dealt with in the PageRank algorithm\r\n\r\n      PageRank algorithm\r\n      - Invented by the founders of Google and is named after Larry Page\r\n      - Variation of it are now used by many large search engines\r\n      - The basic idea is that every page is assigned a score that indicates how important the page is\r\n      - The importance of a page is calculated from the importance of all other pages linking to it\r\n      - The importance score is then used to weight inbound links to a page\r\n\r\n      The Theory\r\n      - The algorithm calculates the probability that someone randomly clicking on links will arrive at a certain page\r\n      - The more inbound links a page has from other popular pages, the more likely it is that someone will visit the page by pure chance\r\n      - If the user keeps clicking forever he will eventually reach every page\r\n      - Since users stop surfing after a while, PageRank has a damping factor of 0.85 indicating that there is a 85% chance that a user will continue clicking from a page\r\n  \r\n  */\r\n\r\n  /* Indexing */\r\n  // Give each word a unique value, e.g. use hash codes\r\n  // Store in a linked list of integers instead so that it has a key\r\n\r\n  /* Searching (when we have index) */\r\n  // When we search for a single word the return is a list containing all documents that has the word\r\n  // Multiple words returns a list of documents that has any of the words in them (perhaps all documents with all the words as well ). Really depends on the purpose of the search engine.\r\n\r\n  /* Ranking */\r\n  // Here comes the sorting of the result...\r\n  // The sort is based on how relevant each document is to the search query (const query)\r\n  // This needs to be done by generating a numeric score for each document's relevance\r\n  // The score is based on a combination of several metrics. The metric can be tweaked (if needed) to hopefully work better for a specific search engine\r\n}\r\n\r\nexport default search\r\n"]},"metadata":{},"sourceType":"module"}